<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "XHTML1-s.dtd" >
<html xmlns="http://www.w3.org/TR/1999/REC-html-in-xml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>
<style>
.bodyContainer {
    font-family: Arial, Helvetica, sans-serif;
    text-align: center;
    padding-left: 32px;
    padding-right: 32px;
}

.notebookFor {
    font-size: 18px;
    font-weight: 700;
    text-align: center;
    color: rgb(119, 119, 119);
    margin: 24px 0px 0px;
    padding: 0px;
}

.bookTitle {
    font-size: 32px;
    font-weight: 700;
    text-align: center;
    color: #333333;
    margin-top: 22px;
    padding: 0px;
}

.authors {
    font-size: 13px;
    font-weight: 700;
    text-align: center;
    color: rgb(119, 119, 119);
    margin-top: 22px;
    margin-bottom: 24px; 
    padding: 0px;
}

.sectionHeading {
    font-size: 24px;
    font-weight: 700;
    text-align: left;
    color: #333333;
    margin-top: 24px;
    padding: 0px;
}

.noteHeading {
    font-size: 18px;
    font-weight: 700;
    text-align: left;
    color: #333333;
    margin-top: 20px;
    padding: 0px;
}

.noteText {
    font-size: 18px;
    font-weight: 500;
    text-align: left;
    color: #333333;
    margin: 2px 0px 0px;
    padding: 0px;
}

.highlight_blue {
    color: rgb(178, 205, 251);
}

.highlight_orange {
    color: #ffd7ae;
}

.highlight_pink {
    color: rgb(255, 191, 206);
}

.highlight_yellow {
    color: rgb(247, 206, 0);
}

.notebookGraphic {
    margin-top: 10px;
    text-align: left;
}

.notebookGraphic img {
    -o-box-shadow:      0px 0px 5px #888;
    -icab-box-shadow:   0px 0px 5px #888;
    -khtml-box-shadow:  0px 0px 5px #888;
    -moz-box-shadow:    0px 0px 5px #888;
    -webkit-box-shadow: 0px 0px 5px #888;
    box-shadow:         0px 0px 5px #888; 
    max-width: 100%;
    height: auto;
}

hr {
    border: 0px none;
    height: 1px;
    background: none repeat scroll 0% 0% rgb(221, 221, 221);
}
</style>
</head>
<body>
<div class='bodyContainer'>
<h1><div class='notebookFor'>Notes and highlights for</div><div class='bookTitle'>The Book of Why (Penguin Science)
</div><div class='authors'>
Pearl, Judea; Mackenzie, Dana
</div></h1><hr/>
<div class='noteText'>First draft of notes - testing export of notes feature</div><hr/>
<h2 class='sectionHeading'>Introduction: Mind over Data</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 5 &middot; Location 147</div><div class='noteText'>every student learns to chant , “ Correlation is not causation . ” With good reason ! The rooster’s crow is highly correlated with the sunrise ; yet it does not cause the sunrise . Unfortunately , statistics has fetishized this commonsense observation . It tells us that correlation is not causation , but it does not tell us what causation is .</h3>
<h3 class='noteHeading'>Note - Page 5 &middot; Location 149</div><div class='noteText'>What is causation?</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 6 &middot; Location 160</div><div class='noteText'>But I hope with this book to convince you that data are profoundly dumb . Data can tell you that the people who took a medicine recovered faster than those who did not take it , but they can’t tell you why .</h3>
<h3 class='noteHeading'>Note - Page 6 &middot; Location 161</div><div class='noteText'>Data is dumb. Why is also equivalent to how should we act on the analysis results</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 9 &middot; Location 211</div><div class='noteText'>One of the crowning achievements of the Causal Revolution has been to explain how to predict the effects of an intervention without actually enacting it .</h3>
<h3 class='noteHeading'>Note - Page 9 &middot; Location 212</div><div class='noteText'>Very handy in LA</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 10 &middot; Location 233</div><div class='noteText'>My emphasis on language also comes from a deep conviction that language shapes our thoughts . You cannot answer a question that you cannot ask , and you cannot ask a question that you have no words for .</h3>
<h3 class='noteHeading'>Note - Page 10 &middot; Location 234</div><div class='noteText'>What do we lack the words foor in LA?</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 11 &middot; Location 256</div><div class='noteText'></h3>
<h3 class='noteHeading'>Note - Page 11 &middot; Location 256</div><div class='noteText'>Inference engine</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 12 &middot; Location 263</div><div class='noteText'>Scientific research always requires simplifying assumptions , that is , statements which the researcher deems worthy of making explicit on the basis of the available Knowledge . While most of the researcher’s knowledge remains implicit in his or her brain , only Assumptions see the light of day and are encapsulated in the model . They can in fact be read from the model , which has led some logicians to conclude that a model is nothing more than a list of assumptions . Computer</h3>
<h3 class='noteHeading'>Note - Page 13 &middot; Location 266</div><div class='noteText'>Assumptions, epistemic cut</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 14 &middot; Location 297</div><div class='noteText'>Notice that the whole notion of estimands and in fact the whole top part of Figure I does not exist in traditional methods of statistical analysis . There , the estimand and the query coincide . For example , if we are interested in the proportion of people among those with Lifespan L who took the Drug D , we simply write this query as P ( D | L ) . The same quantity would be our estimand . This already specifies what proportions in the data need to be estimated and requires no causal knowledge . For this reason , some statisticians to this day find it extremely hard to understand why some knowledge lies outside the province of statistics and why data alone cannot make up for lack of scientific knowledge .</h3>
<h3 class='noteHeading'>Note - Page 15 &middot; Location 303</div><div class='noteText'>Traditional approach issues</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 16 &middot; Location 321</div><div class='noteText'>The hope — and at present , it is usually a silent one — is that the data themselves will guide us to the right answers whenever causal questions come up . I am an outspoken skeptic of this trend because I know how profoundly dumb data are about causes and effects . For example , information about the effects of actions or interventions is simply not available in raw data , unless it is collected by controlled experimental manipulation . By contrast , if we are in possession of a causal model , we can often predict the result of an intervention from hands - off , intervention - free data .</h3>
<h3 class='noteHeading'>Note - Page 16 &middot; Location 325</div><div class='noteText'>Faith in dumb data</h3>
<h2 class='sectionHeading'>1: The Ladder of Causation</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 27 &middot; Location 485</div><div class='noteText'></h3>
<h3 class='noteHeading'>Note - Page 27 &middot; Location 485</div><div class='noteText'>Causal ladder</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 30 &middot; Location 522</div><div class='noteText'>machine learning programs ( including those with deep neural networks ) operate almost entirely in an associational mode . They are driven by a stream of observations to which they attempt to fit a function , in much the same way that a statistician tries to fit a line to a collection of points . Deep neural networks have added many more layers to the complexity of the fitted function , but raw data still drives the fitting process .</h3>
<h3 class='noteHeading'>Note - Page 31 &middot; Location 525</div><div class='noteText'>Machine learning</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 31 &middot; Location 534</div><div class='noteText'>Many scientists have been quite traumatized to learn that none of the methods they learned in statistics is sufficient even to articulate , let alone answer , a simple question like “ What happens if we double the price ? ” I know this because on many occasions I have helped them climb to the next rung of the ladder . Why can’t we answer our floss question just by observation ? Why not just go into our vast database of previous purchases and see what happened previously when toothpaste cost twice as much ? The reason is that on the previous occasions , the price may have been higher for different reasons . For example , the product may have been in short supply , and every other store also had to raise its price . But now you are considering a deliberate intervention that will set a new price regardless of market conditions . The result might be quite different from when the customer couldn’t find a better deal elsewhere . If you had data on the market conditions that existed on the previous occasions , perhaps you could make a better prediction … but what data do you need ?</h3>
<h3 class='noteHeading'>Note - Page 32 &middot; Location 541</div><div class='noteText'>Complexity in analysing rung 2 questions</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 32 &middot; Location 554</div><div class='noteText'>Another popular question at the second level of causation is “ How ? , ” which is a cousin of “ What if we do … ? ” For instance , the manager may tell us that we have too much toothpaste in our warehouse . “ How can we sell it ? ” he asks . That is , what price should we set for it ? Again , the question refers to an intervention , which we want to perform mentally before we decide whether and how to do it in real life . That requires a causal model .</h3>
<h3 class='noteHeading'>Note - Page 32 &middot; Location 557</div><div class='noteText'>How? Is a  key quesfion in LA too </h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 41 &middot; Location 680</div><div class='noteText'>“ Whenever you make an event happen , ” we tell the computer , “ remove all arrows that point to that event and continue the analysis by ordinary logic , as if the arrows had never been there . ”</h3>
<h3 class='noteHeading'>Note - Page 41 &middot; Location 681</div><div class='noteText'>Do, on the graph</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 45 &middot; Location 743</div><div class='noteText'>The main lesson for a student of causality is that a causal model entails more than merely drawing arrows . Behind the arrows , there are probabilities . When we draw an arrow from X to Y , we are implicitly saying that some probability rule or function specifies how Y would change if X were to change . We might know what the rule is ; more likely , we will have to estimate it from data . One of the most intriguing features of the Causal Revolution , though , is that in many cases we can leave those mathematical details completely unspecified . Very often the structure of the diagram itself enables us to estimate all sorts of causal and counterfactual relationships : simple or complicated , deterministic or probabilistic , linear or nonlinear .</h3>
<h3 class='noteHeading'>Note - Page 45 &middot; Location 749</div><div class='noteText'>Meaning of arrows in DAG, probability</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 46 &middot; Location 759</div><div class='noteText'>Decades ’ worth of experience with these kinds of questions has convinced me that , in both a cognitive and a philosophical sense , the idea of causes and effects is much more fundamental than the idea of probability . We begin learning causes and effects before we understand language and before we know any mathematics . ( Research has shown that three - year - olds already understand the entire Ladder of Causation . ) Likewise , the knowledge conveyed in a causal diagram is typically much more robust than that encoded in a probability distribution .</h3>
<h3 class='noteHeading'>Note - Page 46 &middot; Location 762</div><div class='noteText'>Causal …probability, former is more natural</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 47 &middot; Location 770</div><div class='noteText'>The recognition that causation is not reducible to probabilities has been very hard - won , both for me personally and for philosophers and scientists in general .</h3>
<h3 class='noteHeading'>Note - Page 47 &middot; Location 771</div><div class='noteText'>But is it? Bayesian methods and causal inference guyss</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 51 &middot; Location 847</div><div class='noteText'>The main point is this : while probabilities encode our beliefs about a static world , causality tells us whether and how probabilities change when the world changes , be it by intervention or by act of imagination .</h3>
<h3 class='noteHeading'>Note - Page 51 &middot; Location 849</div><div class='noteText'>If we want to help people make decisions then this is absolutely key</h3>
<h2 class='sectionHeading'>2: From Buccaneers to Guinea Pigs: The Genesis of Causal Inference</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 59 &middot; Location 948</div><div class='noteText'>FIGURE 2.2 . The scatter plot shows a data set of heights , with each dot representing the height of a father ( on the x - axis ) and his son ( on the y - axis ) . The dashed line coincides with the major axis of the ellipse , while the solid line ( called the regression line ) connects the rightmost and leftmost points on the ellipse . The difference between them accounts for regression to the mean . For</h3>
<h3 class='noteHeading'>Note - Page 59 &middot; Location 951</div><div class='noteText'>Regression to the mean</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 62 &middot; Location 986</div><div class='noteText'>FIGURE 2.3 . Galton’s regression lines . Line OM gives the best prediction of a son’s height if you know the height of the father ; line ON gives the best prediction of a father’s height if you know the height of the son . Neither is the same as the major axis ( axis of symmetry ) of the scatter plot . ( Source : Francis Galton , Journal of the Anthropological Institute of Great Britain and Ireland [ 1886 ] , 246 – 263 , Plate X . )</h3>
<h3 class='noteHeading'>Note - Page 62 &middot; Location 989</div><div class='noteText'>Galtons regressions lines</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 75 &middot; Location 1194</div><div class='noteText'>FIGURE 2.7 . Sewall Wright’s first path diagram , illustrating the factors leading to coat color in guinea pigs . D = developmental factors ( after conception , before birth ) , E = environmental factors ( after birth ) , G = genetic factors from each individual parent , H = combined hereditary factors from both parents , O , Oʹ = offspring . The objective of analysis was to estimate the strength of the effects of D , E , H ( written as d , e , h in the diagram ) . ( Source : Sewall Wright , Proceedings of the National Academy of Sciences [ 1920 ] ,</h3>
<h3 class='noteHeading'>Note - Page 75 &middot; Location 1199</div><div class='noteText'>Guinea Pigs</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 85 &middot; Location 1357</div><div class='noteText'>R . A . Fisher , the undisputed high priest of statistics in the generation after Galton and Pearson , described this difference succinctly . In 1925 , he wrote , “ Statistics may be regarded as … the study of methods of the reduction of data . ” Pay attention to the words “ methods , ” “ reduction , ” and “ data . ” Wright abhorred the idea of statistics as merely a collection of methods ; Fisher embraced it . Causal analysis is emphatically not just about data ; in causal analysis we must incorporate some understanding of the process that produces the data , and then we get something that was not in the data to begin with . But Fisher was right about one point : once you remove causation from statistics , reduction of data is the only thing left .</h3>
<h3 class='noteHeading'>Note - Page 85 &middot; Location 1362</div><div class='noteText'>Fisher reductionism</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 87 &middot; Location 1403</div><div class='noteText'>In this one sentence Karlin articulates how little had changed from the days of Pearson and how much influence Pearson’s ideology still had in 1983 . He is saying that the data themselves already contain all scientific wisdom ; they need only be cajoled and massaged ( by “ displays , indices , and contrasts ” ) into dispensing those pearls of wisdom . There is no need for our analysis to take into account the process that generated the data . We would do just as well , if not better , with a “ model - free approach . ” If Pearson were alive today , living in the era of Big Data , he would say exactly this : the answers are all in the data . Of course , Karlin’s statement violates everything we learned in Chapter 1 . To speak of causality , we must have a mental model of the real world . A “ model - free approach ” may take us to the first rung of the Ladder of Causation , but no farther .</h3>
<h3 class='noteHeading'>Note - Page 88 &middot; Location 1410</div><div class='noteText'>Stubborn critisism of Wright; need for a model</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 89 &middot; Location 1424</div><div class='noteText'>Wright traces the allure of “ model - free ” methods to their objectivity . This has indeed been a holy grail for statisticians since day one — or since March 15 , 1834 , when the Statistical Society of London was founded . Its founding charter said that data were to receive priority in all cases over opinions and interpretations . Data are objective ; opinions are subjective . This paradigm long predates Pearson . The struggle for objectivity — the idea of reasoning exclusively from data and experiment — has been part of the way that science has defined itself ever since Galileo .</h3>
<h3 class='noteHeading'>Note - Page 89 &middot; Location 1428</div><div class='noteText'>The allure of 'model free' objectivity</h3>
<h2 class='sectionHeading'>4: Confounding and Deconfounding: Or, Slaying the Lurking Variable</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 140 &middot; Location 2164</div><div class='noteText'>This question has bedeviled both theoretical and practical statisticians ; it has been an Achilles ’ heel of the field for decades . That is because it has nothing to do with data or statistics . Confounding is a causal concept — it belongs on rung two of the Ladder of Causation .</h3>
<h3 class='noteHeading'>Note - Page 140 &middot; Location 2166</div><div class='noteText'>Confounding is ncausal</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 145 &middot; Location 2239</div><div class='noteText'>like the image that Fisher Box provides in the above passage : Nature is like a genie that answers exactly the question we pose , not necessarily the one we intend to ask .</h3>
<h3 class='noteHeading'>Note - Page 145 &middot; Location 2240</div><div class='noteText'>The genie</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 151 &middot; Location 2329</div><div class='noteText'>The quantity we observe is the conditional probability of the outcome given the treatment , P ( Y | X ) . The question we want to ask of Nature has to do with the causal relationship between X and Y , which is captured by the interventional probability P ( Y | do ( X ) ) . Confounding , then , should simply be defined as anything that leads to a discrepancy between the two : P ( Y | X ) ≠ P ( Y | do ( X ) ) . Why all the fuss ?</h3>
<h3 class='noteHeading'>Note - Page 151 &middot; Location 2332</div><div class='noteText'>Confounding defined in do calculus</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 164 &middot; Location 2544</div><div class='noteText'>In fact , the full model in their paper has a few more variables and looks like the diagram in Figure 4.7 . Note that Game 5 is embedded in this model in the sense that the variables A , B , C , X , and Y have exactly the same relationships . So we can transfer our conclusions over and conclude that we have to control for A and B or for C ; but C is an unobservable and therefore uncontrollable variable . In addition we have four new confounding variables : D = parental asthma , E = chronic bronchitis , F = sex , and G = socioeconomic status . The reader might enjoy figuring out that we must control for E , F , and G , but there is no need to control for D . So a sufficient set of variables for deconfounding is A , B , E , F , and G . FIGURE 4.7 . Andrew Forbes and Elizabeth Williamson’s model of smoking ( X ) and asthma ( Y ) . In</h3>
<h3 class='noteHeading'>Note - Page 164 &middot; Location 2554</div><div class='noteText'>Causal model of smoking and asthma</h3>
<h2 class='sectionHeading'>5: The Smoke-Filled Debate: Clearing the Air</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 177 &middot; Location 2712</div><div class='noteText'>Epidemiologists in the 1950s faced the criticism that their evidence was “ only statistical . ” There was allegedly no “ laboratory proof . ” But even a look at history shows that this argument was specious . If the standard of “ laboratory proof ” had been applied to scurvy , then sailors would have continued dying right up until the 1930s , because until the discovery of vitamin C , there was no “ laboratory proof ” that citrus fruits prevented scurvy .</h3>
<h3 class='noteHeading'>Note - Page 177 &middot; Location 2715</div><div class='noteText'>Smoking - only statistical proof!</h3>
<h2 class='sectionHeading'>6: Paradoxes Galore!</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 194 &middot; Location 2959</div><div class='noteText'>FIGURE 6.1 . Causal diagram for Let’s Make a Deal . As you can see from Figure 6.1 , Door Opened is a collider . Once we obtain information on this variable , all our probabilities become conditional on this information . But when we condition on a collider , we create a spurious dependence between its parents . The dependence is borne out in the probabilities : if you chose Door 1 , the car location is twice as likely to be behind Door 2 as Door 1 ; if you chose Door 2 , the car location is twice as likely to be behind Door 1 .</h3>
<h3 class='noteHeading'>Note - Page 194 &middot; Location 2964</div><div class='noteText'>Monty  Hall DAG</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 196 &middot; Location 3004</div><div class='noteText'>Our brains are not prepared to accept causeless correlations , and we need special training — through examples like the Monty Hall paradox or the ones discussed in Chapter 3 — to identify situations where they can arise .</h3>
<h3 class='noteHeading'>Note - Page 197 &middot; Location 3006</div><div class='noteText'>Causeless correlations</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 197 &middot; Location 3007</div><div class='noteText'>MORE COLLIDER BIAS : BERKSON’S PARADOX In 1946 , Joseph Berkson , a biostatistician at the Mayo Clinic , pointed out a peculiarity of observational studies conducted in a hospital setting : even if two diseases have no relation to each other in the general population , they can appear to be associated among patients in a hospital . To understand Berkson’s observation , let’s start with a causal diagram ( Figure 6.3 ) . It’s also helpful to think of a very extreme possibility : neither Disease 1 nor Disease 2 is ordinarily severe enough to cause hospitalization , but the combination is . In this case , we would expect Disease 1 to be highly correlated with Disease 2 in the hospitalized population . FIGURE 6.3 . Causal diagram for Berkson’s paradox . By performing a study on patients who are hospitalized , we are controlling for Hospitalization . As we know , conditioning on a collider creates a spurious association between Disease 1 and Disease 2 . In many of our previous examples the association was negative because of the explain - away effect , but here it is positive because both diseases have to be present for hospitalization ( not just one ) . However , for a long time epidemiologists refused to believe in this possibility . They still didn’t believe it in 1979 , when David Sackett of McMaster University , an expert on all sorts of statistical bias , provided strong evidence that Berkson’s paradox is real .</h3>
<h3 class='noteHeading'>Note - Page 198 &middot; Location 3020</div><div class='noteText'>Collider bias by selection</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 199 &middot; Location 3041</div><div class='noteText'>Reichenbach’s error was his failure to consider collider structures — the structure behind the data selection . The mistake was particularly illuminating because it pinpoints the exact flaw in the wiring of our brains . We live our lives as if the common cause principle were true . Whenever we see patterns , we look for a causal explanation . In fact , we hunger for an explanation , in terms of stable mechanisms that lie outside the data . The most satisfying kind of explanation is direct causation : X causes Y . When that fails , finding a common cause of X and Y will usually satisfy us . By comparison , colliders are too ethereal to satisfy our causal appetites . We still want to know the mechanism through which the two coins coordinate their behavior . The answer is a crushing disappointment . They do not communicate at all . The correlation we observe is , in the purest and most literal sense , an illusion . Or perhaps even a delusion : that is , an illusion we brought upon ourselves by choosing which events to include in our data set and which to ignore . It is important to realize that we are not always conscious of making this choice , and this is one reason that collider bias can so easily trap the unwary . In the two - coin experiment , the choice was conscious : I told you not to record the trials with two tails . But on plenty of occasions we aren’t aware of making the choice , or the choice is made for us . In the Monty Hall paradox , the host opens the door for us . In Berkson’s paradox , an unwary researcher might choose to study hospitalized patients for reasons of convenience , without realizing that he is biasing his study .</h3>
<h3 class='noteHeading'>Note - Page 200 &middot; Location 3053</div><div class='noteText'>Etheral and hard to comprehend colliders</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 207 &middot; Location 3166</div><div class='noteText'>FIGURE 6.4 . Causal diagram for the Simpson’s paradox example . For women , the rate of heart attacks was 5 percent without Drug D and 7.5 percent with Drug D . For men , the rate of heart attacks was 30 percent without Drug D and 40 percent with . Taking the average ( because men and women are equally frequent in the general population ) , the rate of heart attacks without Drug D is 17.5 percent ( the average of 5 and 30 ) , and the rate with Drug D is 23.75 percent ( the average of 7.5 and 40 ) .</h3>
<h3 class='noteHeading'>Note - Page 207 &middot; Location 3170</div><div class='noteText'>Simpsons paradox v 1</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 209 &middot; Location 3198</div><div class='noteText'>FIGURE 6.5 . Causal diagram for the Simpson’s paradox example ( second version ) .</h3>
<h3 class='noteHeading'>Note - Page 209 &middot; Location 3199</div><div class='noteText'>Simpsons paaradox v 2</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 211 &middot; Location 3224</div><div class='noteText'>the statistical trends ( either in the aggregated data , the partitioned data , or both ) cannot represent the causal effects . There are , of course , other warning signs of confounding . The aggregated estimate of the causal effect could , for example , be larger than each of the estimates in each of the strata ; this likewise should not happen if we have controlled properly for confounders .</h3>
<h3 class='noteHeading'>Note - Page 211 &middot; Location 3226</div><div class='noteText'>Confounding warnings  signs</h3>
<h2 class='sectionHeading'>7: Beyond Adjustment: The Conquest of Mount Intervention</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 228 &middot; Location 3508</div><div class='noteText'>In 2014 , Adam Glynn and Konstantin Kashin , both political scientists at Harvard ( Glynn subsequently moved to Emory University ) , wrote a prize - winning paper that should be required reading for all quantitative social scientists . They applied the new method to a data set well scrutinized by social scientists , called the Job Training Partnership Act ( JTPA ) Study , conducted from 1987 to 1989 . As a result of the 1982 JTPA , the Department of Labor created a job - training program that , among other services , provided participants with occupational skills , job - search skills , and work experience . It collected data on people who applied for the program , people who actually used the services , and their earnings over the subsequent eighteen months . Notably , the study included both a randomized controlled trial ( RCT ) , where people were randomly assigned to receive services or not , and an observational study , in which people could choose for themselves .</h3>
<h3 class='noteHeading'>Note - Page 229 &middot; Location 3515</div><div class='noteText'>Read this paper - could hold clues for a causal model of retention</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 229 &middot; Location 3516</div><div class='noteText'>FIGURE 7.2 . The basic setup for the front - door criterion .</h3>
<h3 class='noteHeading'>Note - Page 229 &middot; Location 3516</div><div class='noteText'>Front door criterian</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 230 &middot; Location 3525</div><div class='noteText'>FIGURE 7.3 . Causal diagram for the JTPA Study . Comparing Figure 7.2 to Figure 7.3 , we can see that the front - door criterion would apply if there were no arrow from Motivation to Showed Up , the “ shielding ” I mentioned earlier . In many cases we could justify the absence of that arrow . For example , if the services were only offered by appointment and people only missed their appointments because of chance events unrelated to Motivation ( a bus strike , a sprained ankle , etc . ) , then we could erase that arrow and use the front - door criterion .</h3>
<h3 class='noteHeading'>Note - Page 230 &middot; Location 3530</div><div class='noteText'>Causal model comparible to retention</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 231 &middot; Location 3541</div><div class='noteText'>Glynn and Kashin’s work gives both empirical and methodological proof that as long as the effect of C on M ( in Figure 7.2 ) is weak , front - door adjustment can give a reasonably good estimate of the effect of X on Y . It is much better than not controlling for C . Glynn</h3>
<h3 class='noteHeading'>Note - Page 231 &middot; Location 3544</div><div class='noteText'>Usefullness of the front door criterian</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 234 &middot; Location 3590</div><div class='noteText'>For example , Rule 1 says when we observe a variable W that is irrelevant to Y ( possibly conditional on other variables Z ) , then the probability distribution of Y will not change . For example , in Chapter 3 we saw that the variable Fire is irrelevant to Alarm once we know the state of the mediator ( Smoke ) . This assertion of irrelevance translates into a symbolic manipulation : P ( Y | do ( X ) , Z , W ) = P ( Y | do ( X ) , Z )</h3>
<h3 class='noteHeading'>Note - Page 234 &middot; Location 3595</div><div class='noteText'>Do calculus 1</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 234 &middot; Location 3599</div><div class='noteText'>Another legitimate transformation is familiar to us from our back - door discussion . We know that if a set Z of variables blocks all back - door paths from X to Y , then conditional on Z , do ( X ) is equivalent to see ( X ) . We can , therefore , write P ( Y | do ( X ) , Z ) = P ( Y | X , Z )</h3>
<h3 class='noteHeading'>Note - Page 234 &middot; Location 3602</div><div class='noteText'>Do calculus 2</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 234 &middot; Location 3605</div><div class='noteText'>Rule 3 is quite simple : it essentially says that we can remove do ( X ) from P ( Y | do ( X ) ) in any case where there are no causal paths from X to Y . That is , P ( Y | do ( X ) ) = P ( Y ) if there is no path from X to Y with only forward - directed arrows . We can paraphrase this rule is follows : if we do something that does not affect Y , then the probability distribution of Y will not change . Aside from being just as self - evident as Euclid’s axioms ,</h3>
<h3 class='noteHeading'>Note - Page 235 &middot; Location 3611</div><div class='noteText'>Do calculus 3</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 236 &middot; Location 3632</div><div class='noteText'>FIGURE 7.4 . Derivation of the front - door adjustment formula from the rules of do - calculus . In a restaurant the evening before the talk , I had written the proof ( very much like the one in Figure 7.4 ) on a napkin for David Freedman .</h3>
<h3 class='noteHeading'>Note - Page 236 &middot; Location 3635</div><div class='noteText'>Do calc proof example</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 239 &middot; Location 3681</div><div class='noteText'>Even more problems of this sort arise when we consider problems of transportability or external validity — assessing whether an experimental result will still be valid when transported to a different environment that may differ in several key ways from the one studied . This more ambitious set of questions touches on the heart of scientific methodology , for there is no science without generalization . Yet the question of generalization has been lingering for at least two centuries , without an iota of progress . The tools for producing a solution were simply not available . In 2015 , Bareinboim and I presented a paper at the National Academy of Sciences that solves the problem , provided that you can express your assumptions about both environments with a causal diagram .</h3>
<h3 class='noteHeading'>Note - Page 239 &middot; Location 3686</div><div class='noteText'>Transportability</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 249 &middot; Location 3832</div><div class='noteText'>FIGURE 7.9 . General setup for instrumental variables . Because Z and X are unconfounded , the causal effect of Z on X ( that is , a ) can be estimated from the slope rXZ of the regression line of X on Z . Likewise , the variables Z and Y are unconfounded , because the path Z → X ← U → Y is blocked by the collider at X . So the slope of the regression line of Z on Y ( rZY ) will equal the causal effect on the direct path Z → X → Y , which is the product of the path coefficients : ab . Thus we have two equations : ab = rZY and a = rZX . If we divide the first equation by the second , we get the causal effect of X on Y : b = rZY / rZX . In this way , instrumental variables allow us to perform the same kind of magic trick that we did with front - door adjustment : we have found the effect of X on Y even without being able to control for , or collect data on , the confounder , U .</h3>
<h3 class='noteHeading'>Note - Page 250 &middot; Location 3843</div><div class='noteText'>Instrumental variables</h3>
<h2 class='sectionHeading'>10: Big Data, Artifi cial Intelligence, and the Big Questions</h2><h3 class='noteHeading'>Highlight (<span class='highlight_yellow'>yellow</span>) - Page 362 &middot; Location 5605</div><div class='noteText'>primarily limitations that stem from their inability to go beyond rung one of the Ladder of Causation . This limitation does not hinder the performance of AlphaGo in the narrow world of go games , since the board description together with the rules of the game constitutes an adequate causal model of the go - world . Yet it hinders learning systems that operate in environments governed by rich webs of causal forces , while having access merely to surface manifestations of those forces . Medicine , economics , education , climatology , and social affairs are typical examples of such environments . Like the prisoners in Plato’s famous cave , deep - learning systems explore the shadows on the cave wall and learn to accurately predict their movements .</h3>
<h3 class='noteHeading'>Note - Page 362 &middot; Location 5610</div><div class='noteText'>Complexity and causality</h3>
</div> 
</body> 
</html> 
